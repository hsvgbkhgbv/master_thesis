

    
@misc{safety,
    title={IEEE 1547-2018 - IEEE Standard for Interconnection and Interoperability of Distributed Energy Resources with Associated Electric Power Systems Interfaces}, url={https://standards.ieee.org/standard/1547-2018.html}, 
    year={2018},
    month={Feb}}

    
@article{opf_intro,
    author = {Stephen Frank and Steffen Rebennack},
    title = {An introduction to optimal power flow: Theory, formulation, and examples},
    journal = {IIE Transactions},
    volume = {48},
    number = {12},
    pages = {1172-1197},
    year  = {2016},
    publisher = {Taylor & Francis},
    doi = {10.1080/0740817X.2016.1189626},
    URL = { 
        https://doi.org/10.1080/0740817X.2016.1189626},
    eprint = { 
        https://doi.org/10.1080/0740817X.2016.1189626}
}
    
    
@misc{python_web,
    title={Welcome to Python.org},
    url={https://www.python.org/},
    journal={Python.org}
}



@ARTICLE{pandapower,
    author={L. Thurner and A. Scheidler and F. Schäfer and J. Menke and J. Dollichon and F. Meier and S. Meinecke and M. Braun},
    journal={IEEE Transactions on Power Systems},
    title={pandapower — An Open-Source Python Tool for Convenient Modeling, Analysis, and Optimization of Electric Power Systems},
    year={2018},
    month={11},
    volume={33},
    number={6},
    pages={6510-6521},
    doi={10.1109/TPWRS.2018.2829021},
    ISSN={0885-8950}
}

@misc{keras_chollet2015,
  title={Keras},
  author={François Chollet and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@book{Sutton1998,
  added-at = {2019-02-05T11:01:00.000+0100},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/analyst},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {2018 book reference reinforcement-learning},
  publisher = {The MIT Press},
  timestamp = {2019-02-05T11:01:02.000+0100},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018},
  isbn = {978-0262039246}
}

@InProceedings{pmlr-v32-silver14,
  title = 	 {Deterministic Policy Gradient Algorithms},
  author = 	 {David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {387--395},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {06},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/silver14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/silver14.html},
  abstract = 	 {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.}
}

@article{DBLP:journals/corr/LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.02971},
  archivePrefix = {arXiv},
  eprint    = {1509.02971},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Andrychowicz_HER,
  author    = {Marcin Andrychowicz and
               Filip Wolski and
               Alex Ray and
               Jonas Schneider and
               Rachel Fong and
               Peter Welinder and
               Bob McGrew and
               Josh Tobin and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {Hindsight Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1707.01495},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.01495},
  archivePrefix = {arXiv},
  eprint    = {1707.01495},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AndrychowiczWRS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{schaul15_goal_states,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Tom Schaul and Daniel Horgan and Karol Gregor and David Silver},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {7},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters theta. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}

@misc{energifakta_norge, title={Kraftproduksjon}, url={https://energifaktanorge.no/norsk-energiforsyning/kraftforsyningen/},
journal={Energifakta Norge},
urldate = {2019-03-02}
}

@online{plotly,
author = {Plotly Technologies Inc.}, title = {Collaborative data science}, publisher = {Plotly Technologies Inc.}, address = {Montreal, QC},
year = {2019},
url = {https://plot.ly}}

@online{enlopy,
author = {K. Kavvadias},
title = {Enlopy: Python toolkit for energy load time series},
url = {http://github.com/kavvkon/enlopy}
}

@online{pandapower_website,
    title={About pandapower},
    url={https://www.pandapower.org/about},
    urldate = {2019-04-17}
}


@online{silver_course,
    author = {David Silver},
    title= {UCL course on reinforcement learning},
    year = {2015},
    url = {http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html},
    urldate = {2019-04-18}
}

@article{value_based_policy_Nachum,
  author    = {Ofir Nachum and
               Mohammad Norouzi and
               Kelvin Xu and
               Dale Schuurmans},
  title     = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1702.08892},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.08892},
  archivePrefix = {arXiv},
  eprint    = {1702.08892},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/NachumNXS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{solar_data,
  author = {SoDa},
  title = {SoDa - Solar radiation data},
  year = 2019,
  url = {http://www.soda-pro.com/about-us},
  urldate = {2019-04-08}
}


@article{virtual_inertia,
author = {Tamrakar, Ujjwol and Shrestha, Dipesh and Maharjan, Manisha and Bhattarai, Bishnu and Hansen, Timothy and Tonkoski, Reinaldo},
year = {2017},
month = {06},
pages = {654},
title = {Virtual Inertia: Current Trends and Future Directions},
volume = {7},
journal = {Applied Sciences},
doi = {10.3390/app7070654}
}

@inproceedings{synchronous_regions,
author = {Gerasimov, Krum and Gerasimov, Konstantin and Nikolaev, Nikolay},
year = {2014},
month = {09},
pages = {},
title = {Advanced Tools for Stability Improvement of Interconnected Electric Power Systems}
}


@article{active_network_management,
  author    = {Quentin Gemine and
               Damien Ernst and
               Bertrand Cornélusse},
  title     = {Active network management for electrical distribution systems: problem
               formulation and benchmark},
  journal   = {CoRR},
  volume    = {abs/1405.2806},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.2806},
  archivePrefix = {arXiv},
  eprint    = {1405.2806},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GemineEC14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{somer1017,
  author    = {Oscar De Somer and
               Ana Soares and
               Tristan Kuijpers and
               Koen Vossen and
               Koen Vanthournout and
               Fred Spiessens},
  title     = {Using Reinforcement Learning for Demand Response of Domestic Hot Water
               Buffers: a Real-Life Demonstration},
  journal   = {CoRR},
  volume    = {abs/1703.05486},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.05486},
  archivePrefix = {arXiv},
  eprint    = {1703.05486},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SomerSKVVS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{demand_response_definition,
title = "A summary of demand response in electricity markets",
abstract = "This paper presents a summary of Demand Response (DR) in deregulated electricity markets. The definition and the classification of DR as well as potential benefits and associated cost components are presented. In addition, the most common indices used for DR measurement and evaluation are highlighted, and some utilities' experiences with different demand response programs are discussed. Finally, the effect of demand response in electricity prices is highlighted using a simulated case study.",
keywords = "Demand response, Electricity markets, Price elasticity, Real time pricing",
author = "Albadi, {M. H.} and El-Saadany, {E. F.}",
year = "2008",
month = "11",
doi = "10.1016/j.epsr.2008.04.002",
language = "English",
volume = "78",
pages = "1989--1996",
journal = "Electric Power Systems Research",
issn = "0378-7796",
publisher = "Elsevier BV",
number = "11",
}

@article{cigre,
  author  = {Convener Kai Strunz et. al}, 
  title   = {Benchmark Systems for Network Integration of Renewable and Distributed Energy Resources},
  journal = {Electra},
  year    = 2014,
  number  = 273,
  pages   = {85-89},
  month   = 4
}

@inproceedings{thermo_q_learning,
title = "Reinforcement Learning for Demand Response of Domestic Household Appliances",
abstract = "With today’s electricity grid being penetrated with more and more intermittent energy sources, the need arises to match demand for electricity with electricity generation, in order to maintain the stability of the grid. Demand response has been proposed as a mechanism that aims to solve this problem, by stimulating the shift of electricity demand towards production peaks. In previous work individual devices were trained using fitted-Q iteration to shift the consumption of electrical energy towards low-price periods, while still guaranteeing user comfort. This paper shows that controlling multiple devices with one independent agent per device is more cost-effective compared to a centralized agent. The paper expands the setting with an energy consumption constraint on the level of the household. The objective is to spread the energy consumption of the devices during low price periods as much as possible in order to avoid overloading the grid. Preliminary results for both centralized and independent learners show that these agents are capable of reducing the amount of constraint violations minimally but fail to reduce violations to zero.",
author = "Mathieu Reymond and Christophe Patyn and Roxana Radulescu and Ann Nowe and Geert Deconinck",
year = "2018",
month = "7",
day = "16",
language = "English",
pages = "18--25",
booktitle = "Proceedings of the Adaptive Learning Agents Workshop 2018 (ALA-18)",
}