\documentclass[class=book, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}


\begin{document}
%\chapter{Reinforcement learning}
\section{Intuitive introduction to reinforcement learning}
Reinforcement learning is an algorithm that learns through trial and error. The system consists of an agent that observes a state and responds to that by taking an action. Simply put, the agent will get a positive reward when it takes good actions and negative rewards for bad actions. When the agent takes a bad action, it will be less likely to chose that action again later. Similarly, when it gets a reward it will likely chose a similar action given the same observed state. By letting the agent see many states and explore different actions, it can eventually learn a policy that maximises expected future rewards. 

This algorithm is similar to how humans and animals learn. Imagine a dog seeing its owner holding a bag of treats. Obviously, the dog is keen on getting the treats, but is not sure what to do. The dog sees that the owner is putting his hand in front of its nose and yelling some command, but does not quit understand what to do. So it simply tries doing something. First, it might try to lean forward and smell the hand. Sadly, this does not result in any treat. Therefore, it continuous to try different actions, until it eventually happens to lift its front paw in the hand of the owner. At last, it receives a tasty treat from the owner. It has learned what action to take to get a treat. Next, the owner might rotate its arm in front of the dog. The dog might try to lift its front paw again, since that worked last time. Sadly, it does not get a reward this times. Therefore, it starts to explore new actions until it after some time tries to spin around. Again, it receives a treat. It has now learned that simply raising its front paw does not always result in a treat. It has to evaluate its observation before taking an action. 

The dog training is similar to an reinforcement learning algorithm. The dog is the agent that tries to figure own what actions to do, while the owner is the reward system. The advantage of the reinforcement algorithm is that it does not need a physical reward, and the agent can experiment much more quickly than a dog can. 

\section{Reinforcement algorithms}
Algorithms in machine learning and artificial intelligence are often divided into either supervised or unsupervised learning. Supervised learning is an algorithm using input data and labelled output data (target). The algorithm tries to map the input to the target in a manner that generalises well to unseen input data. Examples of supervised learning are regression and classification algorithms. Unsupervised learning is algorithms attempting to find structure in unlabelled data. Examples of such are clustering and anomaly detection. The terms \textit{supervised} and \textit{unsupervised} do not describe well the mechanisms of reinforcement learning algorithms. An reinforcement learning agent learns from interacting with an environment and receiving rewards based the action it takes. The agent's goal is not to use labelled data in some sense or explicitly finding general structures in the data. As a result, reinforcement learning is considered to be a category of its own\cite{Sutton1998}. 

\section{Markov decision process}

A Markov decision process is a mathematical framework describing sequential decision making and interaction with an environment, where the outcome can be stochastic. This is precisely what
a general reinforcement algorithm attempts to do. The set of possible actions and states are respectively called the action space $\mathcal{A}$ and state space $\mathcal{S}$. For some reinforcement learning tasks, such as chess, the action space depends on the state $s$. For instance, it is not allowed to castle if an opponents piece is attacking some of the squares over which the king will move. In such cases, the action space $\mathcal{A}(s)$ is given by the state. The action space in an electric power system is dependent on the state, for instance if a power plant is out of operation. The environment starts at $t=0$ and is described by a state $s_{0} \in \mathcal{S}$. The agent preforms some action $a_{0}\in \mathcal{A}$ and receives a reward $r_{1}\in \mathcal{R} \subseteq \mathbb{R} $ based on how "good" that action is. The action $a_{0}$ interacts with the environment and gives a new state $s_{1}$. This starts the sequence of states, actions and rewards.


\begin{equation}
   \begin{aligned}\label{eq:theory:trajectory}
s_{0},a_{0},r_{1},s_{1}, a_{1},r_{2},s_{2},...
\end{aligned} 
\end{equation}

The interaction between the agent and environment is visualised in figure \ref{fig:theory:markov_decision_process} as a feedback loop.

\begin{figure}[ht!]
    \center
    \includegraphics[height=3.5cm, width=10cm]{figures/markov_decision_processs.JPG}
    \caption[size = 9]{Interaction in a Markov decision process. Source: Sutton \cite{Sutton1998}}
    \label{fig:theory:markov_decision_process}
\end{figure}



The goal at each time step $t$ is to maximise the rewards in the future. How to formally define the reward maximising criterion depends on the nature of the task. Some tasks, such as playing a video game, are called episodic and have well-defined boundaries for start and end state. On the other hand, the electric power system is a continuous task that never should end if the agent does its job. For continuous tasks, let the discounted return $g_{t}$ at time $t$ be defined as 


\begin{equation}
   \begin{aligned}\label{eq:theory:discounted_reward}
g_{t} = r_{t+1} + \gamma r_{t+2} + \gamma^{2} r_{t+3} + ...
= \sum_{k=t}^{\infty} \gamma^{k-t}r_{k+1}
\end{aligned} 
\end{equation}
where $r_{t}$ is the reward received after action $a_{t}$ and $\gamma \in [0,1]$ is the discount factor. The goal of the agent at every time step $t$ could be to maximise the discounted return $g_{t}$.



\section{Value and policy functions}
It is hard to talk about reinforcement learning algorithms without introducing the value function. Simply put, the value function evaluates how good an action $a$ is in a given state $s$. More concretely, the value function $V_{\pi}(s)$ is the expected discounted reward $g_{t}$ as defined by \eqref{eq:theory:discounted_reward} given state $s$ and following the policy $\pi$.

\begin{equation}
   \begin{aligned}\label{eq:theory:value_function}
V_{\pi}(s) 
= \mathbb{E}_{\pi}[g_{t}| s=s_{t}]
= \mathbb{E}_{\pi}[ r_{t+1} + \gamma r_{t+2} + \gamma^{2} r_{t+3} + ...|s=s_{t}]
\end{aligned} 
\end{equation}

Another central function is the action-value function $Q_{\pi}(s,a)$, which quantifies the expected discounted return given the action $a_{t}$ in state $s_{t}$ and that the policy $\pi$ is followed thereafter. 

\begin{equation}
   \begin{aligned}\label{eq:theory:action_value_function}
Q_{\pi}(a_{t},s_{t}) 
= \mathbb{E}_{\pi}[g_{t}|a=a_{t} ,s=s_{t}]
= \mathbb{E}_{\pi}[ r_{t+1} + \gamma r_{t+2} + \gamma^{2} r_{t+3} + ...|a=a_{t} ,s=s_{t}]
\end{aligned} 
\end{equation}

The policy $\pi$ of the agent decides what action to take in a given state. The policy can both be deterministic and stochastic. A deterministic policy maps a given state to the same action every time, while a stochastic policy maps the state to a probability distribution over the action space. For the deterministic case, the policy function $\pi$ is given by


\begin{equation}
   \begin{aligned}\label{eq:theory:policy_function_deterministic}
\pi(s) = a
\end{aligned} 
\end{equation}
where $a$ is the action chosen by the policy. The stochastic case 


\begin{equation}
   \begin{aligned}\label{eq:theory:policy_function_stochastic}
\pi(a|s) = p_{a}
\end{aligned} 
\end{equation}
where $p_{a}$ is the probability of choosing action $a$ given the policy $\pi$.


The reinforcement algorithms used in this thesis are restricted to policy-gradient methods. These are methods that approximate the policy function using a neural network and updates its parameters using gradients. 

\section{Off-policy and on-policy}
A problem that arises when constructing a reinforcement learning algorithm is how to both be able to exploit a good policy and at the same time explore new policies. If an agent always follows its policy and picks the action it believes is the best, it will never explore new and perhaps better approaches to solve a problem. A solution to this problem is two have two different policies. One policy is called the target policy that is to be the optimal solution, while the other is called the behaviour policy and is for exploration of new behaviours\cite{Sutton1998}. A reinforcement learning algorithm using this approach is said to be learning off-policy.

\section{Deep deterministic policy gradient}


\end{document}

