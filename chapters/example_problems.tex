\documentclass[class=book, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}

\begin{document}
\section{Load balancing in a two bus system}
An agent using the deep deterministic policy gradient (DDPG) algorithm was given a simple load balancing task on the power network shown in figure \ref{fig:examples:two_bus}.\cite{Sutton1998}

\begin{figure}[ht!]
    \center
    \subimport{../}{circuits/two_bus.tex}
    \caption[size = 9]
    {Simple two bus system connected by a line. \textit{P} and \textit{Q} are the active and reactive power flow, \textit{R} and \textit{X} is the resistance and reactance of the line, \textit{U} is the voltage and \textit{I} is the current flowing}    \label{fig:examples:two_bus}
\end{figure}

The agent controls the active power production at bus 1, and has to tune it so that bus 2 receives the desired load. For simplicity, the demanded load at bus 2 is constant at 1300 kW. The voltages magnitudes at each bus is locked to 1 p.u, while the reactive power injections and voltage angle can vary. The reward $r_{t}$ is defined as the negative of the absolute deviation between the received and desired load at bus 2. The idea is that the agent should be encouraged to move closer to the desired load. After training the.

\begin{tikzpicture}
\begin{axis}
\addplot table  [x=prod, y=busload, col sep=comma, smooth,mark=none] {data/two_bus_load_balancing.csv};

\addplot table  [x=prod, y=cost1, col sep=comma, smooth,mark=none] {data/two_bus_load_balancing.csv};
\end{axis}
\end{tikzpicture}



\end{document}