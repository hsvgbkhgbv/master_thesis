\documentclass[class=book, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\pgfplotsset{compat=1.15}

\begin{document}
\chapter{Conclusion and future work}
The main goal in this thesis is to see if reinforcement learning and demand response can be used to safely operate a power grid with distributed solar energy production. The motivation is that costly upgrades of the electric transmission infrastructure can be avoided if the algorithm succeeds. The following research question was formulated in chapter \ref{chapter:introduction}: 

\begin{displayquote}
\textbf{RQ}: Is the deep deterministic policy gradient algorithm able to reduce the number of safety violations in a grid with high peak solar power production and high peak demand by the means of demand response? 
\end{displayquote}
The short answer to the research question is yes. The agent was able reduce the number of safety violations by 10 \% by manipulating the power consumption in the net. However, investigating the results revealed that the reinforcement algorithm only avoids safety violations in hours of peak demand and actually produced more violations during hours of peak solar production. In total, there are more violations during peak demand than during peak solar production, so the net effect is a decrease in the number of violations. As discussed in chapter \ref{chapter:discussion}, the behaviour of the algorithm is found to quite primitive, and there is room for improvement. This thesis have not tried to tune the model in anyway, and have analysed the results based several arbitrary initial decisions. Therefore, it is natural to assume that improvements can be found, for instance by increasing the training time. 

The implementation of the power system environment are based on several simplifications, such as a constant power factor, equal demand signals and constant flexibility in the net. This is done so it is possible to analyse the behaviour of the algorithm. The implementations can be extended to be more in accordance with a real power system and tested for different combinations of hyperparameters. The agent's action space can also be extended to control transformers, switches and charging of storage units. This is a way to handle the challenges that comes with increased amount of distributed solar production in the electrical power net. Reinforcement learning is a particularly attractive approach for solving this task because the environment can be simulated efficiently, providing the agent with many experience from which it can learn. A successful reinforcement learning algorithm can accelerate the incorporation of more renewable energy into the power mix, as costly and time-consuming upgrades of infrastructure can be avoided. This is of great utility, not only from a power system perspective, but also as a measure to achieve the goals of the Paris agreement.

\end{document}