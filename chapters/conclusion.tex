\documentclass[class=book, crop=false, 11pt]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{pgfplotstable}

\pgfplotsset{compat=1.15}

\begin{document}
\chapter{Conclusion and future work}
The main goal in this thesis is to see if reinforcement learning and demand response can be used to safely operate a power grid with distributed solar energy production. The motivation is that costly upgrades of the electric transmission infrastructure can be avoided if the algorithm succeeds. The following research question was formulated in chapter \ref{chapter:introduction}: 

\begin{displayquote}
\textbf{RQ}: Is the deep deterministic policy gradient algorithm able to reduce the number of safety violations in a grid with high peak solar power production and high peak demand by the means of demand response? 
\end{displayquote}
The short answer to the research question is yes. The agent was able reduce the number of safety violations by 10 \% by changing the power consumption in the net. However, investigating the results revealed that the reinforcement algorithm only avoids safety violations in hours of peak demand and actually produced more violations during hours of peak solar production. In total, there are more violations during peak demand than during peak solar production, so the net effect is a decrease in the number of violations. As discussed in chapter \ref{chapter:discussion}, the behaviour of the algorithm is found to be quite primitive, and there is room for improvement. This thesis has not performed an extensive tuning of the model, and have analysed the results based several arbitrary initial hyperparameters. Therefore, it is natural to assume that improvements can be found, for instance by increasing the training time. 

The implementation of the power system environment is based on several simplifications, such as a constant power factor, equal demand signals and constant flexibility in the net. This is done so it is possible to analyse the behaviour of the algorithm. The implementations can be extended to be more in accordance with a real model for demand response and tested for different combinations of hyperparameters. The agent's action space can also be extended to control transformers, switches and charging of storage units. This is a way to handle the challenges that comes with increased amount of distributed solar production in the electrical power net. Reinforcement learning is a particularly attractive approach for solving this task because the environment can be simulated efficiently, providing the agent with many experience from which it can learn. A successful reinforcement learning algorithm managing a demand response program can accelerate the incorporation of more renewable energy into the power mix, as costly and time-consuming upgrades of infrastructure can be avoided. This is of great utility, not only from a power system perspective, but also as a measure to achieve the goals of the Paris agreement.

\section*{Future work}
There are several natural next steps to continue the work presented in this thesis. 

\subsection*{Pretraining}
The reinforcement agent in this thesis learns entirely from self-play and receives no input of how it should behave. However, the deep deterministic policy gradient method can be pretrained on a dataset with behaviour that is known to be good. Pretraining gives the agent a good starting point from which it can start experimenting with different behaviours and find proper strategies. This is especially attractive since the action space is very large, which means that a lot of training time is used to explore basic behaviour.

\subsection*{Hyperparameter search}
There are many hyperparameters that can be tuned in reinforcement learning, as in most machine learning algorithms. This thesis has not performed for an optimal hyperparameter search. One key parameters that should be tested more thoroughly is the training time, as neural network networks are good at continuously learning from new data, and the action space is large.     
\subsection*{Realistic demand response model}
The model of the demand response program used in this thesis is based on several simplifications. The power system is assumed to offer a demand flexibility that is constant and symmetric in all hours. Concretely, the power consumption can in every hour be changed continuously in the interval [-10\%,10\%]. Realistically, changing the consumption affects the future flexibility in the system. In addition, the trained agent can change the consumption freely without any cost in this thesis. This is unrealistic, since people are not going to change their consumption patterns without an economic compensation. Future work should have a more realistic modelling of a demand response program. This would result in the need for more long term planning because an action greatly affects the future possible actions. Reinforcement algorithms are well suited for tasks that require farsighted strategies. 
\subsection*{More control variables}
The action in the reinforcement algorithm is to change consumption in the grid. A natural extension of this is to let the agent control more elements, such as tap-changing transformers, and charging of batteries. Both transformers and batteries can be controlled using \texttt{pandapower}, the Python power flow simulation tool used in this thesis.
\subsection*{Finer time scale}
The trained reinforcement agent performs an action every hour. Therefore, it is not able to tackle problems in real-time by activating flexibility. Instead, it is appropriate for planing the activation of flexibility in the next hours. A finer time-resolution, for instance every minute, could be implemented in a demand response program that can operate close to real-time.
\subsection*{Wind power}
Wind power is a variable renewable energy source that is not consider in this thesis. Consequently, only a solar forecasts is sent to the reinforcement agent. An extension of the setup can integrate wind power in the network as well, and test how the reinforcement agent tackles different sources of renewable energy production.
\subsection*{Other reinforcement algorithm}
The Python code developed for activation of flexibility of the follows the structure of a general \texttt{gym} environment in Python, and can consequently be applied to different reinforcement algorithms without doing any modification of the code. The Python toolkit \texttt{stable\_baseline} offers several different possible reinforcement learning algorithms, such as Proximal Policy Optimization and Soft Actor Critic.


\end{document}