\documentclass[class=book, crop=false, 11pt]{standalone}
\usepackage[utf8]{inputenc}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkefølge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{makecell}
\usepackage[font=small,labelfont=bf]{caption}

\pgfplotsset{compat=1.15}

\begin{document}
\chapter{Problem Description}\label{chap:problem_description}
Solar power production occurs during day-time and is, not surprisingly, controlled by the sun. As photovoltaic modules become more prevalent in residential areas, they will grow to a size where they produce a significant amount of power that must be transported out on the grid around noon. Later in the afternoon, power must be imported from the external grid because the solar power production is low when residential areas consume power. A normal day will therefore have an import and export period that can challenge the grid in terms of power capacity. A consequence of this is that safety bounds for both voltage and line capacity are in danger of being violated. A strategy for avoiding such problems is called demand response. Albadi and El-Saadany give the following definition of demand response \cite{demand_response_definition}:

\begin{displayquote}
Demand response can be defined as the changes in electricity usage by end-use customers from their normal consumption patterns in response to changes in the price of electricity over time. Further, DR can be also defined as the incentive payments designed to induce lower electricity use at times of high wholesale market prices or when system reliability is jeopardised. DR includes all intentional electricity consumption pattern modifications by end-use customers that are intended to alter the timing, level of
instantaneous demand, or total electricity consumption.
\end{displayquote}

The idea is to modify the demand of electric power such that the size of the peak power in network is reduced. An electric transmission system must be designed to withstand the peak powers, although the network only operates in this period a small proportion of the time. As an example, the municipality Hvaler in southern Norway multiplies its population during vacations, which means that the capacity of the transmission lines to the islands of Hvaler must be built to withstand a very rare peak power situation. To avoid congestion in the grid, the network operator in Hvaler has introduced a power term in the electricity bill, to give an incentive to the consumers to change their consumption pattern \cite{hvaler}. Avoiding congestion in the transmission network is not the only positive consequences of demand response. Vázquez-Canteli and Nagy list the following advantages\cite{vazquez2019reinforcement}:

\begin{itemize}
\item Improved grid stability due to increased demand flexibility. 
\item Shift of peak demand towards periods of peak renewable energy
generation.
\item Lower thermal costs and electricity prices. Since the peak to average
ratio of the demand decreases, less peaking plants need to be operated. 
\item Reduction of the investments in generation, transmission, and distribution assets, which are sized to meet peak demand.
\item Lower capacity reserves requirements.
\item Reduced energy bills for consumers. 
\end{itemize}


Demand response can be categorised into two main programs: Price-Based Programs (PBP) and Incentive-Based Programs (IBP) \cite{demand_response_definition}. PBP and IBS are also respectively called system-led and market-led programs, which reflect the main mechanism for achieving demand response. In classical IBP for demand control, costumers are given some sort of participation payment, such as a discount rate \cite{demand_response_definition}. There is also a market based IBP, which compensates the costumers based on how much and when they participate. This thesis will not go into the details of the different demand response programs, but instead focus on the implementation of a reinforcement algorithm under an ideal assumption where the agent can change power consumption without a cost. This is done so that it is possible to evaluate the reinforcement agent solely based on its safety behaviour. In other words, the goal is to investigate if the agent safely can control the grid using an ideal demand response program, where flexibility is a free resource. A cost of changing the demand is presented as a term in the reward function in section \ref{section:reward}, but is not used in the trained reinforcement algorithm.    

The scope of this thesis is to continually control the absolute demand at different buses in a power grid, given some interval of flexibility at that bus. It might seem strange to use continuous control since power consuming units generally can not be set to an arbitrary power level. The nature of a power component is binary: it either consumes power or it does not consume power. Consequently, a valid question is whether a continuous control setup even is realisable in a real power system. Although individual power consuming units are binary, a large collection of units connected to a bus can be approximated as continuous. Imagine 1000 households connected to a bus and that each household has several power consuming components that can be controlled, such as electric vehicles, water heater and heat pumps. Assuming these units have some flexibility in terms of operation time, it is possible to coordinate all these devices in such a way that the power consumption appear continuous in some interval at the bus. A coordinator that does this in a power system is is called a flexibility aggregator \cite{koliou2014_aggregator}.

This thesis will assume the aggregator exists, so the centralised agent can modify the power consumption continuously in some interval of flexibility. For instance, when the agent wants to decrease the consumption by 2 MW at a bus, there exists a system that turns off electric equipment in households connected to that bus such that the total power change is 2 MW.  The advantage of continuous control is that the action space of the algorithm is dramatically reduced, as there is only one action per bus in the grid. Controlling individual components in a power grid quickly gives a very large and impractical action space since the number of possible actions doubles for every additional control device. In addition, Vázquez-Canteli and Nagy recommended in their literature review of demand response and reinforcement learning that there should be more case studies using modern algorithms such as deep deterministic policy gradient (DDPG), which have a continuous action space \cite{vazquez2019reinforcement}.


The specific setup in this thesis is based on several assumptions for a hypothetical future power grid. The electrical power grid used is constructed by the International Council on Large Electric Systems (CIGRE) as a benchmark network that can be used for analysis of DER integration \cite{cigre}. The network is predefined in \texttt{pandapower}, the Python module used for power flow calculations, and can be defined with different renewable energy sources connected at different buses. The grid used is visualised in figure \ref{fig:problem:cigre_network}. 

\begin{figure}[ht]
    \center
    \includegraphics[height=14cm, width=13.5cm]{figures/cigre_network_mv_der.png}
    \caption  {CIGRE network with solar and wind power that is used in the reinforcement learning algorithm \cite{cigre}}
    \label{fig:problem:cigre_network}
\end{figure}


The left-most feeder has several solar producing units connected and wind power connected to bus 7. From a power flow perspective, there is no difference between a wind farm (without voltage regulation) and solar production since they are both modelled as static generators (PQ-node). For simplicity, the wind farm connected at bus 7 is assumed to be solar in this thesis, i.e its power production follows the sun profile through the day. The time resolution in this task is chosen to be hourly. In other words, the demand and solar production at every bus is updated every hour, and assumed constant in between hours. Naturally, the solar intensity varies continuously in an hour, and average values do not tell if or when clouds are blocking the sun. Because of this, the timescale is not short enough to help solve immediate problems that require actions within seconds or minutes, but can be used to plan on an hourly time scale. 

There exists demand response programs for large power-consuming industries in several European countries, but programs that aggregate the consumption of residential costumers are not implemented in large scale \cite{koliou2014_aggregator}. However, the emerge of the Internet of things is enabling communication and coordination between small power consuming equipment in residences, which can be used for demand control. Therefore, the consumption pattern in this thesis will follow the pattern of residential consumers. The daily demand profile is generated using \texttt{enlopy}, a Python toolkit for energy demand time series \cite{enlopy}. The demand signal is generated based on data of the energy consumption of private households. 

A data set with satellite-derived solar irradiance in central Norway is used as input to define the production of solar power \cite{solar_data}. Figure \ref{fig:problem:solar_data} plots a time series of the solar irradiance, and a couple of example days. The solar data is scaled so the maximum value is 1.

\begin{figure}[ht]
    \center
    \includegraphics[height=9cm, width=13.5cm]{figures/solar_data.png}
    \caption  {The solar irradiance data used in the reinforcement model. The period is 2004-02-11 to 2004-10-03, generated from SoDa \cite{solar_data}. (lon,lat) = (63.395,10.381) }
    \label{fig:problem:solar_data}
\end{figure}


The signal of solar production and power demand is assumed equal at all buses and scaled up based on the nominal values for consumption at each bus. Note that the nominal values vary a lot from bus to bus. For instance, if the demand signal is 0.5, the consumption at each bus is determined by multiplying 0.5 with their respective nominal power that is predefined in \texttt{pandapower}. The same is true for the generation of solar power. This reduces the state space, as there is no need for a unique demand forecast for all the loads.  For simplicity, the power factor is assumed constant at every bus. In other words, if the consumption of active power increases by 1 \%, then the reactive power consumption also increases by 1\%. The power factor at each load is the same as the default values in the CIGRE network. The nominal values of the solar production are intentionally amplified so that the safety margins for line current and voltage magnitudes in the grid frequently are violated if no actions are taken. The solar producing units do not output any reactive power, only active power.

It is assumed that the loads at all buses have some flexibility in terms of power consumption, say 10 \% of the actual demand, that can be centrally controlled at every hour. The reinforcement agent can modify the power consumption independently between the different buses. For instance, it can increase the consumption by 5 \% at one bus, and decrease it by 2\% at another bus in the same hour. The absolute change in consumption at each load in the net is found by multiplying the demand change by the forecasted power consumption in that hour. A significant simplification in the simulation is that the flexibility is assumed constant in all hours, which is far from a realistic scenario. For instance, decreasing the consumption in an hour corresponds to turning off electrical equipment, and that equipment cannot be turned off again in the next hour. By doing this the agent is given a simpler environment to work in. If a reinforcement algorithm is to be successful in a realistic modelling of flexibility, it should be successful in a simplified model as well. This thesis explores the simplified model.

The CIGRE network consists of several power components, such as transformers, lines, switches and loads. Table \ref{table:cigre_components} summarises the different components in the network. 

\begin{table}[ht]
\centering
\caption{Component in the CIGRE network}
\label{table:cigre_components}
\begin{tabular}{l|ll}

Component  & Symbol & Amount 
\\ 
\hline
Bus & N & 15 \\
Switch & - & 8 \\
Static generator & G & 9 \\ 
Line & L & 15 \\
Load & F & 18 \\
Transformer &- & 2

 \\
\hline
\end{tabular}
\end{table}
Table \ref{table:cigre_components} shows that there are 18 loads and 15 buses in the system. The reason is that there are several load elements connected to some buses which makes the number of loads greater than the number of buses. The net power consumption at a bus is found by summing over the consumption of all the loads connected to that bus. 



\section{State space}\label{section:problem:state_space}
It is not obvious how to design the state space for the agent. This section presents several spaces that could be useful for the reinforcement algorithm. Not all of the presented spaces are used in the trained reinforcement learning agent which is analysed in chapter \ref{chapter:results}.

Let $\mathcal{S}_{\textrm{sun}} \subseteq  \mathbb{R}^{H}$ be space of the forecasted solar irradiance in the grid \textit{H} hours into the future. This gives the agent information about the coming solar production and can be used for finding control strategies. For instance, consider an hour with high production of solar power, where the transmission lines are overloaded. If the agent sees that the forecast predicts a dip in solar production in the next hour due to clouds, it can increase the local power consumption at solar producing buses, to relieve the overloaded power lines. It can then safely decrease the consumption the next hour, since the sun is blocked by the clouds and thereby restore the total energy consumption over the two hours. This is an example of a desired behaviour that the agent ideally should find. The buses are assumed to be geographically close, so the solar irradiance is the same everywhere. If a cloud blocks the sun at bus 1, then it also blocks the sun at all the other buses in the system. This significantly reduces the state space compared to a unique forecast for every bus. 

Let $\mathcal{S}_{\textrm{demand}} \subseteq \mathbb{R}^{F\cdot H}$ be the space of the forecasted power demand $H$ hours into the future in a grid with $F$ flexible loads. It is also possible to let the total demand in the market represent the demand state. A problem with this approach is that it only partially describes the demand situations in the market since the agent does not receive information about demand at individual loads. An advantage, however, is that the size of the a state vector from  $\mathcal{S}_{\textrm{demand}}$ is much smaller than a corresponding vector for all loads. Specifically, the CIGRE network that is used has 18 loads, which would make the state vector 18 times greater.


Let $\mathcal{S}_{\textrm{bus}} \subseteq \mathbb{R}^{4N}$ be the space representing the state of all the buses in the net. Specifically

\begin{equation}
   \begin{aligned}
   \label{eq:problem:bus_state}
\mathcal{S}_{\textrm{bus}} = \{|U_{i}|, \delta_{i}, P_{i}, Q_{i} |\; i = 1,...,N\}
    \end{aligned} 
\end{equation}

where $|U_{i}|,\delta_{i}, P_{i}$ and $Q_{i}$ respectively are the voltage magnitude, voltage phase angle, active power injection and reactive power injection at bus $i$ in a $N$-bus system. This could give information to the agent about how stressed the system is. For instance, large values for active power $P_{i}$ and voltage magnitude $|U_{i}|$ indicate a stressed situation at bus $i$ possibly due to high solar production. 



The agent should also get information that ensures that the consumption at a load merely is shifted and not altered in absolute magnitude. In other words, a state that contains information about the energy imbalance in the grid. A positive energy imbalance means that the agent has forced the loads to use more energy than the real demand. If the agent changes a load by -1 MW for an hour, the agent should ideally increase the load by 1 MW some time not far into the future. Gemine et al. did this by making a commitment when the demand is changed at a load. When a load is changed, it follows a predefined modulation curve for $T_{d}$ time steps (for instance 4 steps) \cite{active_network_management}. The modulation signal is constructed so that it sums to zero over the time period $T_{d}$, which guarantees that the total energy consumption of the bus is constant. Note that Gemine et al. did not implement a reinforcement learning algorithm. The state vector could indicate in what modulation step a load is at. For instance, if the modulation started at $t_{0}$ for a load and the current time step is $t_{0} + 3$, then the load component of this vector is 3. The agent is then always fed with a signal that tells it the commitment stage of that load. The desired action of the agent at a time step in the commitment period will simply be ignored so that the load follows the modulation signal. This has the drawback that the agent's actions frequently are overridden. Given a modulation period of \textit{k} steps, the agent desired action is only performed $1/k$ times. Hopefully the agent will pick up on this through the commitment state vector. Formally, the commitment space $\mathcal{S}_{\textrm{commitment}} \subseteq \mathbb{R}^{F}$ can be defined as

\begin{equation}
   \begin{aligned}
   \label{eq:problem:commitment_state}
\mathcal{S}_{\textrm{commitment}} = \{ c_{i} | \;i = 1,...,F\}
    \end{aligned} 
\end{equation}
where $c_{i} \in \{0,1,..,k\}$ is the commitment stage of flexible load $i$ in a $k$-period commitment period in a grid with $F$ flexible loads.

The agent does not have to follow a predefined commitment signal to ensure that the power consumption is shifted, but can instead be penalised for changing the total consumption during a day. The agent must then be fed some information about the energy imbalance at the flexible loads, i.e if the loads have consumed more or less energy than the original demand over a period. This could be done by introducing the energy imbalance $B_{i,t}$ of flexible load $i$ at time $t$.

\begin{equation}
   \begin{aligned}
   \label{eq:problem:balance}
    B_{i,t} = \sum_{k=1}^{24}\Delta P_{i,t-k}
    \end{aligned} 
\end{equation}
$\Delta P_{i,t-k}$ is the change in power consumption $k$ time steps before $t$ at flexible load \textit{i}. Simply put, the imbalance $B_{i,t}$ expresses if a flexible load is in balance in terms of energy consumption over the last 24 hours. A positive imbalance means the flexible loads have consumed more power than they originally planned. The goal is to have the imbalance close to zero, which means that power consumption has been shifted and not altered in absolute magnitude. Let $\mathcal{S}_{\textrm{imbalance}} \subseteq \mathbb{R}^{F}$  be the state space representing the power imbalance, defined by 
\begin{equation}
   \begin{aligned}
   \label{eq:problem:imbalance_state}
\mathcal{S}_{\textrm{imbalance}} = \{ B_{i} |\; i = 1,...,F\}
    \end{aligned} 
\end{equation}
where $B_{i}$ is the energy imbalance at flexible load $i$ defined by \eqref{eq:problem:balance}. Table \ref{table:state_spaces} summarises the different state spaces that are presented in this section. The reinforcement learning algorithm will be tested with a subset of the spaces, and the final state variable is constructed by concatenating the individual state vectors.  

\begin{table}[ht]
\centering
\caption{State spaces that can be used in the reinforcement algorithm. $H$ is the forecast horizon, i.e the number of hours into the future the agent receives a forecast. $N$ and $F$ are respectively the number of buses and flexible loads in the net. $r_{i}$ and $d_{i}$ are, respectively, the forecasted solar irradiance and power demand $i$ hour in the future. $|U_{i}|,\delta_{i}, P_{i}$ and $Q_{i}$ are respectively the voltage magnitude, voltage phase angle, active power injection and reactive power injection at bus $i$. $c_{i}$ and $B_{i}$ are respectively the commitment stage and energy imbalance of flexible load \textit{i}}
\label{table:state_spaces}
\begin{tabular}{l|lll}

State space  & Symbol & Size & Definition
\\ 
\hline
Solar forecast      & $\mathcal{S}_{\textrm{sun}}$& H  &  $\{r_{j} |\;j=1,...,H\}$
\\ 

Demand forecast    & $\mathcal{S}_{\textrm{demand}}$ &$H\cdot F$ &$\{d_{j,i} |\;j=1,...,H,i=1,...,F\}$  \\ 
Bus state & $\mathcal{S}_{\textrm{bus}}$ & $4N$ &$\{\delta_{i}, P_{i}, Q_{i}, |U_{i}| |\; i = 1,...,N\}$\\

Commitment state &$\mathcal{S}_{\textrm{commitment}}$& $F$  &$\{ c_{i} |\; i = 1,...,F\}$ \\
 
Imbalance state &$\mathcal{S}_{\textrm{imbalance}}$& $F$  &$\{ B_{i} |\; i = 1,...,F\}$ \\

\hline
\end{tabular}
\end{table}
\section{Action space}
The reinforcement agent can manipulate the power demand at flexible loads in the power grid. The amount of flexibility at a load is defined to be certain percent of the forecasted power demand. It is assumed that the flexibility is symmetric so the demand can both be tuned up and down. The action space in a power grid with $F$ flexible load  $\mathcal{A}  \subseteq \mathbb{R}^{F}$ is defined as
\begin{equation}
   \begin{aligned}
   \label{eq:problem:action_space}
\mathcal{A}= \{a_{i} | \;i = 1,...,F\}
    \end{aligned} 
\end{equation}
where $a_{i} \in [-1,1]$ is the activation at flexible load $i$ in the power grid. $a_{i} = 1$ means that load $i$ increases its active power consumption as much as possible. The change in demand $\Delta P_{i}$ is then scaled up according to the action signal $a_{i}$ by

\begin{equation}
   \begin{aligned}
   \label{eq:problem:update_demand}
    \Delta P_{i}& = f_{i}a_{i}P_{i,\textrm{forecast}} \\
    P_{i}& \leftarrow P_{i} + \Delta P_{i}
    \end{aligned} 
\end{equation}
where $f_{i}$ is the flexibility, $P_{i}$ is the active power demand and $P_{i,\textrm{forecast}}$ is the forecasted power demand at load $i$. Note that every flexible load has a unique demand change that is independent of the demand change at the other loads. The resulting reactive power demand is found by multiplying the active power with a load constant, because the loads are assumed to have a constant power factor.

It is possible to include more control variables that the agent can control. For instance, the reinforcement agent could control the tap position of the transformer and the switches in the system. There is also a CIGRE benchmark network in pandapower that has storage units in the power net. A possible extension is to let the agent control the charging and discharging of storage units.

\section{Reward function}\label{section:reward}
The reward function is a central element in any reinforcement algorithm. The terms \textit{cost} and \textit{reward} are used interchangeably in this thesis. In all cases the reward is the negative of the cost. By this definition, maximising the reward is the same as minimising the cost. 

The reward function should give a signal that is used to reinforce "good" behaviour. The goal of the agent is avoid violations of safety margins for voltage and current in the power grid. Gemine et al. formulate a reward function aimed to safely operate a power grid at a low cost, where they punish the agent proportionally to the violation of safety margins \cite{active_network_management}. There are multiple cost terms that can be included, and this section will present several of these. 

There are safety margins in terms of voltage magnitudes in an electric transmission system. In the Norwegian power system, there are regulations that state that the voltage can vary by +/- 10 \% of nominal levels for voltage levels below 1 kV \cite{lovdata}. This thesis will assume a stricter safety region around nominal voltage values, motivated by the higher voltage level in the CIGRE network (22 kV). The default safety margin for voltage is set to +/- 5 \% deviation from nominal voltage. Let $C_{\textrm{voltage},i}$ be the cost for violating voltage margins at bus $i$


\begin{equation}
   \begin{aligned}
   \label{eq:problem:voltage_margins_cost}
    C_{\textrm{voltage},i} = \max(0,|U_{i}| - U_{\textrm{upper}}) + \max(0,U_{\textrm{lower}}- |U_{i}|)
    \end{aligned} 
\end{equation}
where $U_{\textrm{upper}}$ and $U_{\textrm{lower}}$ are the upper and lower per-unit voltage limit respectively. The voltage cost for a node in the grid with a safety margin of +/- 5 \%  is visualised in figure \ref{fig:problem:voltage_cost}.
\begin{figure}[ht]
    \center
\includegraphics[height=8cm, width=12cm]{figures/voltage_cost.png}
    \caption{Voltage cost at a bus in the power system where the lower and upper voltage limits respectively are 0.95 pu and 1.05 pu }
    \label{fig:problem:voltage_cost}
\end{figure}

Let $C_{\textrm{current},i}$ be the cost of violating current margins in line $i$

\begin{equation}
   \begin{aligned}
   \label{eq:problem:current_margins_cost}
    C_{\textrm{current},i} = \max(0,|I_{i}| - I_{\textrm{upper}})
    \end{aligned} 
\end{equation}
where $I_{\textrm{upper}}$ is the per unit upper current loading limit in lines. Note that the current loading is not measured in ampere, but is the percentage of used capacity in the transmission line. The current cost is therefore weighted by the capacity of a line. The current cost for a line is plotted in figure \ref{fig:problem:current_cost} for an upper current loading limit of 90 \%.

\begin{figure}[ht]
    \center
\includegraphics[height=8cm, width=12cm]{figures/current_cost.png}
    \caption  {Current cost for a line in the power system where the upper current loading is 90 \%}
    \label{fig:problem:current_cost}
\end{figure}
Costumers must be given an incentive to change their consumption pattern in a in a realistic modelling of a demand response program. In other words, costumers should be economically compensated when their flexibility is activated. Let the activation cost $C_{\textrm{activation},i}$ be defined as

\begin{equation}
   \begin{aligned}
   \label{eq:problem:activation_cost}
    C_{\textrm{activation},i} = \lambda |\Delta P_{i}|
    \end{aligned} 
\end{equation}
where $|\Delta P_{i}|$  is the absolute change in power consumption at flexible load \textit{i} and $\lambda$ is the flexibility price at the time of activation \footnote{The currency is arbitrary, and does not affect the cost as it can be scaled by the weight of activation}. In other words, the cost is the same if the consumption is increased or decreased.

It is desired that the flexible loads consume more power during periods with heavy solar production. In addition, flexible loads should use less energy during peak demand, both to reduce violations in the grid and to ensure that the total energy consumption in a day is not altered. The agent should be punished for having a large imbalance. Consequently, it should be penalised for taking an action that increases the absolute balance quantity. Let $C_{\textrm{imbalance},i}$ be the energy imbalance cost at time $t$ for flexible load \textit{i}.

\begin{equation}
   \begin{aligned}
   \label{eq:problem:balance_cost}
    C_{\textrm{imbalance},i} = |B_{i,t}| - |B_{i,t-1}|
    \end{aligned} 
\end{equation}
where the energy imbalance $B_{i,t}$ is given by equation \eqref{eq:problem:balance}. The agent is penalised if the result of an action increases the energy imbalance in absolute magnitude.  

The total cost of an agent at each step is defined as a linear combination over the voltage, current, activation and imbalance cost 

\begin{equation}
   \begin{aligned}
   \label{eq:problem:total_cost}
    C = 
    \kappa_{1} \sum_{i=1}^{F}C_{\textrm{activation},i} + 
    \kappa_{2} \sum_{i=1}^{L}C_{\textrm{current},i} +
    \kappa_{3} \sum_{i=1}^{N}C_{\textrm{voltage},i} +
    \kappa_{4} \sum_{i=1}^{L}C_{\textrm{imbalance},i} 
    \end{aligned} 
\end{equation}
Where \textit{L}, \textit{F} and \textit{N} are the number of lines, flexible loads and buses respectively. The weights $\kappa_{i}$ can be tuned, based on what the desired behaviour of the agent is. If safety margins are most important, the weights for cost of activation and imbalance should be small. Note that it is possible to discard a cost completely by setting its weight equal to zero. Lastly, the cost must be turned into a reward that a reinforcement algorithm can use. The reward $R$ is simply defined to be the negative of the cost

\begin{equation}
   \begin{aligned}
   \label{eq:problem:total_reward}
    R = - C
    \end{aligned} 
\end{equation}
where the total cost $C$ is defined by \eqref{eq:problem:total_cost}. Table \ref{table:reward_terms} summarises the different costs and their definition. 

\begin{table}[ht]
\centering
\caption{Cost terms that can be used in the reinforcement algorithm. The subscripts 'upper' and 'lower' means the upper and lower safety margins. $\lambda$ is the unit cost for activation of a flexible load ($\pounds/MWh$). $\Delta P_{i}$ is the change in consumption of flexible load \textit{i}. $B_{i,t}$ is the daily energy imbalance as defined in \eqref{eq:problem:balance}}
\label{table:reward_terms}
\begin{tabular}{l|ll}

Cost  & Definition & Comment
\\ 
\hline
Voltage &
$\max(0,|U_{i}| - U_{\textrm{upper}}) + \max(0,U_{\textrm{lower}}- |U_{i}|)$ &
Violation of voltage margin
\\
Current &
$\max(0,|I_{i}| - I_{\textrm{upper}})$&
Violation of current margin
\\
Activation &
$\lambda |\Delta P_{i}|$&
Activating flexibility
\\
Imbalance &
$|B_{i,t}|- |B_{i,t-1}|$&
\makecell[l]{Changing daily energy \\consumption}
\\
\hline
\end{tabular}
\end{table}

\section{Playing an episode}
This section will describe how the the state of the electric grid is updated in an episode. The reinforcement agent is each hour given a state that represents the system, which includes a forecast for power demand and solar irradiance. Naturally, no forecasts are perfect, so the actual power demand and solar irradiance should deviate from the predicted values. This is done by adding a noise term to the forecasted values. The noise terms for the demand and solar irradiance are assumed to follow a Gaussian distribution with mean 0 and a standard deviation that is proportional to the forecast in that hour. The hyperparameters $\sigma_{\textrm{solar}}$ and $\sigma_{\textrm{demand}}$ determine the uncertainty in the forecasts, and the default values are set to 3 \% in both cases.

The reinforcement agent evaluates a state $s$ and picks an action $a$ to perform in each hour. The action determines the change in power consumption at the flexible loads in the power grid. The power flow equations are ready to be solved when the power demand resulting from the action is computed. There are no voltage regulating generators in the CIGRE network. Consequently, all the buses in the network are modelled as PQ-buses, except for the external grid (slack bus) where active power $P$ and voltage angle $\delta$ are known. After the power flow calculations have been performed, the reward is calculated and used to evaluate the action of the agent. Finally, the forecasts for demand and solar irradiance are updated and the state $s_{t+1}$ for the next hour is found. This concludes the processes involved in one time step of the reinforcement model. The learning process is summarised in pseudo code in algorithm \ref{problem:algorithm_playing_episode} 


\begin{algorithm}[H]
\SetAlgoLined
Select state space $\mathcal{S}$, flexibility $f \in [0,1]$, reward function $r$ 
 \\


 \For{episode 1:M}{
  Randomly select start day and start hour\\
  Receive initial state $s_{1}$
  
  \For{t 1:T}{
Let the reinforcement agent select an action $a_{t}$ based on $s_{t}$
\\
Set global solar irradiance $r$ based on forecasted solar irradiance $r_{\textrm{forecast}}$
\begin{equation}
   \begin{aligned}\label{eq:problem:set_solar_irradiance}
  r = r_{\textrm{forecast}}(1 + \epsilon), \;\; \epsilon \sim \mathcal{N}(0,\sigma_{\textrm{solar}})
    \end{aligned} 
\end{equation}

\For{Static generator 1:G}{
 Update solar production $P_{\textrm{sun}}$ based on nominal value $S_{N}$
\begin{equation}
   \begin{aligned}\label{eq:problem:set_solar}
  P_{\textrm{sun}} = r S_{N}
    \end{aligned} 
\end{equation}
} 

\For{Load 1:F}{
 Set power demand $P$ based on forecasted power demand $P_{\textrm{forecast}}$
\begin{equation}
   \begin{aligned}\label{eq:problem:set_demand_for}
  P = P_{\textrm{forecast}}(1 + \epsilon), \;\; \epsilon \sim \mathcal{N}(0,\sigma_{\textrm{demand}})
    \end{aligned} 
\end{equation}
}


\For{Load 1:F}{
Update power demand $P$ based on the agent's action $a$ and flexibility $f$
\begin{equation}
   \begin{aligned}
   \label{eq:problem:update_demand_algorithm}
    P \leftarrow P + a f P_{\textrm{forecast}}
    \end{aligned} 
\end{equation}
}

Solve power flow equations
\\
Calculate reward with reward function $r$
\\
Find next state $s_{t+1}$


   }{
   
  }
 }
 \caption{Process for updating power demand and solar production in the electrical power grid}
 \label{problem:algorithm_playing_episode}
\end{algorithm}


\end{document}