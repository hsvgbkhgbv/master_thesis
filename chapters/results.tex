\documentclass[class=book, crop=false, 11pt]{standalone}
\usepackage[utf8]{inputenc}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[font=small,labelfont=bf]{caption}


\pgfplotsset{compat=1.15}

\begin{document}
\chapter{Results}\label{chapter:results}

This chapter presents results from a model trained using deep deterministic policy gradient (DDPG).

\section{Feasibility}
It is important to investigate the potential effect demand response has on current loading and voltage magnitudes before a model is evaluated. It is not given that it is physically feasible avoid safety violations in critical periods by modifying the power consumption in grid. Certainly, the voltage magnitude at buses in the net will decrease when the consumption is increased, but how much? This section will show that changing the consumption indeed can affect the voltage and current enough to reduce the number of violations in CIGRE network.
\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/demand_and_solar.png}
    \caption {Mean hourly power demand curve and solar irradiance}
    \label{fig:results:demand_and_solar}
\end{figure}

As mentioned in chapter \ref{chap:problem_description}, the main challenges for the network is that large amounts of power must imported from and exported out to the external grid in a normal day. Figure \ref{fig:results:demand_and_solar} plots the daily mean solar irradiance signal and power demand used for training the reinforcement agent. The first period that is critical for the grid is when the solar irradiance is at its maximum. 


\begin{figure}[ht]
    \center
\includegraphics[height=8cm, width=12cm]{figures/increase_demand_current.png}
    \caption {Current capacity in a critical line when increasing the power consumption with 25 \% in all hours and regular situation. The line connecting bus 1 and 2 in figure \ref{fig:problem:cigre_network} is showed}
    \label{fig:results:increase_demand_current}
\end{figure}

Figure \ref{fig:results:increase_demand_current} illustrates the effect of activating flexibility (increasing consumption) has on the current in a critical line. The consumption of active power is increased by 25 \% for all the loads in every hour. The first and second peak in plot correspond, respectively, to the solar peak and the power demand peak. It is clear from the first peak that increasing the consumption decreases the line current. This is because the buses in this period generate so much solar power that they act as producers and not consumers. Increasing the consumption means that less power needs to be transported out to the external grid, which decreases the line current. As a result, the desired behaviour in periods with high solar irradiation is to increase the consumption.

The effect of increasing consumption in the second peak is the opposite of that in the first peak. This is because the loads now act as consumers since there is no solar production. The active power must therefore be imported from the external grid. Increasing the consumption in this period simply corresponds to drawing even more power from the external grid, which in turn increases the line current loading. The desired behaviour in this period is therefore to lower the power consumption, since this would decrease the current in the line. 

Figure \ref{fig:results:decrease_demand_voltage} illustrates the effect activation of flexibility has on voltage magnitude at a critical bus bar. In this case, the consumption of active power is decreased by 25 \% at all loads in the network. There are two critical periods during a normal day for this case as well. The effect of decreasing the consumption in periods with high solar is that the voltage magnitude increases. This can be seen between around hour 13 in figure \ref{fig:results:decrease_demand_voltage}. The buses are in this period acting as a producers because of the excess solar power that is transported out to the external grid. Generally, higher production at a bus bar means higher voltage magnitude. Reducing the consumption means that even more power from solar production must be sent out to the external grid, raising the voltage magnitude ever further. Consequently, decreasing the consumption in periods with high solar irradiance is not a desired behaviour. 

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/decrease_demand_voltage.png}
    \caption {Voltage magnitude at critical bus when the power consumption is reduced with 25 \% in all hours and regular situation. Bus bar 9 in \ref{fig:problem:cigre_network} is used}
    \label{fig:results:decrease_demand_voltage}
\end{figure}


The second critical period in a normal day can be seen around hour 20 in figure \ref{fig:results:decrease_demand_voltage}. At this point, there is no solar power production, and power must be imported from the external grid to meet the demand. The voltage magnitude is now lower than its nominal value because the loads consume. Generally, higher consumption means lower voltage magnitude. Therefore it is better to reduce the consumption of power in this period because the voltage magnitudes will be closer to nominal values.

To summarise, it is physically feasible impact the current and voltage magnitudes in the net such that safety violations can be avoided or reduced by the means of demand response. The desired behaviour in terms of both current and voltage safety is to increase consumption in periods with high solar irradiance and decrease it during peak demand.


\section{Simulation 1 - Free activation} \label{section:result:config1}
In this simulation, a reinforcement agent with a flexibility of 10 \% is trained with a reward function that does not include the cost of activation. This is not a realistic case of demand response, since households that offer flexibility should be compensated for altering their energy profile. However, it shows how an agent would activate flexibility if there was no direct cost associated with altering the power consumption. The agent is penalised for changing the total daily energy demand in the power net. Note that it is not penalised for changing the daily consumption at individual loads as long as the total consumption in the network is preserved. The specific reward terms with weights are shown in table \ref{table:results:reward_formulation1}.

\begin{table}[h]
\centering
\caption{Reward terms and weights for formulation 1}
\begin{tabular}{l|ll}

Cost  & Weight & Comment
\\ 
\hline
Voltage &
1 &
Per-unit values
\\
Current &
$10^{-2}$ &
Percentage of max current 
\\
Activation &
0&
No activation cost
\\
Imbalance &
$10^{-4}$&
Units of energy imbalance is kWh
\\
\hline
\end{tabular}
\label{table:results:reward_formulation1}
\end{table}


\begin{table}[h]
\centering
\caption{State used in formulation 1}
\begin{tabular}{l|lll}

State space  & Size & Comment
\\ 
\hline
Solar forecast      &  4  &  4 hour solar forecast
\\ 

Demand forecast    &4 & 4 hour demand forecast 
\\ 
Imbalance state & 1  & Total energy imbalance in the net
\\
\hline
\end{tabular}
\label{table:results:state_formulation1}
\end{table}


\begin{figure}[h]
    \center
    \includegraphics[scale=0.7]{figures/configuration1.png}
    \caption {Activation of flexibility at the different flexible loads of the trained agent throughout a day. The red line is the solar irradiance during the day, which is the same for all loads. The actions in green is the activation of flexibility. Action = 1 means that the flexible load increases its power consumption with 10 \%, while action = -1 means that it decreases its power consumption with 10 \%. The solar irradiance is unitless, and plotted to show the relation between the action and solar profile.}
    \label{fig:results:configuration1}
\end{figure}

The state space is constructed to be as small as possible. The state is represented by a 4 hour forecast for both solar irradiance and active power demand. The power demand as a percentage of nominal consumption is assumed equal at all the flexible load, so there is not an individual power demand for each load. In addition to the forecasts, the total power imbalance for the whole power network is included. Table \ref{table:results:state_formulation1} summarises the state space

The DDPG agent was trained for 100 000 time steps, with no uncertainty in the forecasts. In other words, the agent receives perfect information of the solar and demand situation in the next hours. This can be seen as a model training on historical data for solar irradiance and demand. The once training is done, the agent is tested in an environment with uncertainty in both the solar and demand forecasts. The forecast error follows a Gaussian distribution with standard deviation equal to 3 \% of the forecasted value. A complete summary with all hyperparameters used can be found in appendix \ref{section:apendix_config1}.

Figure \ref{fig:results:configuration1} visualises the actions of the trained agent throughout an arbitrary day (24 hours) together with the solar irradiance. Simply put, it is desired that the actions follow the curve of the solar irradiance around noon, and goes down in the afternoon. 

\begin{figure}[h]
    \center
\includegraphics[scale=0.7]{figures/configuration1_follows_sun.png}
    \caption {Action of the agent at load 12, and solar irradiance during a day. The consumed power is increased in periods with high solar production}
    \label{fig:results:configuration1_follows_sun}
\end{figure}

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/configuration1_negative_actions.png}
    \caption {Action of the agent at load 8 and solar profile during a day}
    \label{fig:results:configuration1_negative_actions}
\end{figure}


By inspecting figure \ref{fig:results:configuration1} it is clear that the agent activates flexibility in times with much solar production for several of the loads. The plot \texttt{load = 12} in is shown alone in figure \ref{fig:results:configuration1_follows_sun}, and clearly follows this pattern. The agent increases its power consumption during periods with high solar production, and reduces it later during peak demand.



The plot for \texttt{load 8} is shown alone in figure \ref{fig:results:configuration1_negative_actions}, and shows some peculiar behaviour. Clearly, the actions do not follow the solar profile that day, but are negative most of the day. This behaviour could be a result of how the reward function is defined in this experiment. The agent is penalised according to the total energy imbalance in the system, and not at individual loads. As a result, if the energy imbalance is +1 MWh at one load and -1 MWh at another load, they perfectly cancel each other, and the agent is not penalised. From the agent's perspective, the system is in energy balance, although individual loads may have a large absolute energy imbalance. This illustrates the problem with constructing state variable that accounts for the system as a whole, and not individual loads. The agent uses the same strategy consistently at this load. It can appear as if this load functions as an energy balance, whose main job is to ensure that the total power imbalance in the grid is kept as small as possible. However, the behaviour of the agent gives a negative energy imbalance in the long term, as seen in figure \ref{fig:results:configuration1_energy_imbalance}. The agent controls a 200 hour long episode, and it is evident that the energy imbalance quickly decreases and reaches an equilibrium around -13 MWh. The agent prefers to decrease the total consumption in the system. It may indicate that the reward function needs further parameter tuning.
\begin{figure}[H]
    \center
\includegraphics[height=8cm, width=12cm]{figures/configuration1_imbalance.png}
    \caption {Total energy imbalance in the system during a 200 hour episode controlled by the trained agent}
    \label{fig:results:configuration1_energy_imbalance}
\end{figure}

\subsection{Voltage violations}
The reward function has an energy imbalance term during training that is meant to incentives the agent to shift energy consumption. However, the main goal of the agent is to keep the values for current and voltage within safety margins. Therefore, it is interesting to see the rewards of the trained model when only the voltage and current terms are considered. In other words, the agent is only penalised for violating safety margins for current and voltage in the grid. The trained model was tested for 500 episodes each consisting of 200 steps and evaluated
in terms of voltage penalty and current penalty independently. The statistics for the hours with a non-zero reward using only voltage reward are presented in table \ref{table:results:configuration1_reward_500_episodes}. Note that every non-zero reward corresponds to a violation of voltage margin because only the voltage term is used to calculate the reward. The trained agent reduces the number of voltage safety margin violations by 18 \% from 8804 to 7253. Note that there is no guarantee that it is physically feasible to satisfy voltage safety margins given a demand flexibility of 10 \%. In addition to reducing the number of violations, the trained agent reduces the mean voltage penalty by 8\%.
\begin{table}[h]
\center
\caption{Statistics of the voltage penalty given to the trained agent and no agent scenario over 500 episodes with 200 hours each}
\begin{tabular}{l|rrrrrrr}
         & count    & mean  & std   & 25\%  & 50\%  & 75\%  & max   \\
\hline
Agent    & 7253 & 0.035 & 0.037 & 0.006 & 0.021 & 0.052 & 0.237 \\
No agent & 8804 & 0.037 & 0.039 & 0.007 & 0.023 & 0.057 & 0.227 \\
\hline
\end{tabular}
\label{table:results:configuration1_reward_500_episodes}
\end{table}

Figure \ref{fig:results:config1_voltage_boxplot} shows the voltage penalty distribution for the trained agent and no agent scenario for hours with violations of voltage safety margins. These hours are termed critical since there would be voltage violations if no actions were taken. The trained agent is better in critical situations, as more of the penalties are centred closer to zero.

\begin{figure}[H]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_voltage_boxplot.png}
    \caption {Box plot of the voltage penalty distribution of the trained agent and no agent scenario for critical hours where safety margins are violated.}
    \label{fig:results:config1_voltage_boxplot}
\end{figure}

The statistics presented in table \ref{table:results:configuration1_reward_500_ep_preventive} show that the mean voltage penalty for the trained agent is reduced by 24 \% in critical hours.


\begin{table}[h]
\center
\caption{Statistics of the voltage penalty given in critical hours to the trained agent and no agent scenario over a 500 episodes simulation with 200 hours each}
\begin{tabular}{l|rrrrrrr}
         & count    & mean  & std   & 25\%  & 50\%  & 75\%  & max   \\
\hline
Agent    & 8804 & 0.028 & 0.036 & 0.000 & 0.014 & 0.044 & 0.237 \\
No agent & 8804 & 0.037 & 0.039 & 0.007 & 0.023 & 0.057 & 0.227 \\
\hline
\end{tabular}
\label{table:results:configuration1_reward_500_ep_preventive}
\end{table}



All votlage penalties in the no agent scenario for critical hours are sorted and plotted in figure \ref{fig:results:config1_sorted_voltage}. The corresponding voltage penalties given to the trained agent are also plotted. This is done to see if the trained agent is better in very critical periods with severe voltage penalties, or if it performs well overall. The plot shows that the trained agent for the most part receives lower voltage penalties regardless of the size of the violation. Still, there are occasions where the trained agent is above the no agent scenario in figure \ref{fig:results:config1_sorted_voltage} which corresponds to periods where the trained agent aggravates the voltage magnitudes. The trained agent makes matters worse in 40\% of the critical hours, and increases the mean penalty with 29 \% in those hours. However, the trained agent improves the voltage margins in 60 \% of the critical hours, where it decreases the mean voltage penalty by 57\%.



\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_sorted_voltage.png}
    \caption {Voltage penalties given to the trained agent and no agent scenario in critical hours. The penalties are sorted for the no agent scenario from least to most severe}
    \label{fig:results:config1_sorted_voltage}
\end{figure}


Figure \ref{fig:results:config1_improvement_voltage} shows for each hour of a day a box plot of the difference between the voltage reward given to the trained agent and no agent scenario in critical hours. This plot illustrates the improvements of the trained agent, measured in terms of the increase in voltage reward. Evidently, the trained agent is only improving the voltage situations in the afternoon, in periods of peak demand. This explains the times the agent makes matters worse in figure \ref{fig:results:config1_sorted_voltage}. 40 \% of the voltage violations occur before peak demand hours (18 pm), which is the same as the proportion of the critical hours where the trained agent makes matters worse. 

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_improvement_voltage.png}
    \caption {Hourly box plots of the difference in voltage rewards between the trained agent and no agent scenario in critical hours. Positive values means that the trained agent is better}
    \label{fig:results:config1_improvement_voltage}
\end{figure}

Until now, the the trained agent has been evaluated in critical hours, where there would be a safety violation in the no agent scenario. However, the electric grid is within safety margins 91 \% of the time in the 500 episode simulation. It is very important that the trained agent finds appropriate behaviour in both non-critical and critical hours. Investigating the voltage penalty given to the agent in non-critical hours reveals that the agent pushes the voltages out of the safety margins 0.6 \% of the time. The worst of these violations is -0.004 in magnitude, which is negligible  compared the violations in critical hours. In other words, the agent behaves well in non-critical hours in terms of voltage. 


\subsection{Current violations}\label{section:config1:current_violations}
The trained agent is tested in a 500 episode simulation, each consisting of 200 hours, and evaluated in terms of violations of current safety margins in the power lines. Specifically, the current cost $C_{\textrm{current}}$ for a line carrying a current $I$ with capacity $I_{\textrm{upper}}$ is $C_{\textrm{current}} = \max(0,I - I_{\textrm{max}})$ where $I_{\textrm{upper}}$ is set to 90 \% of the line capacity. The total current cost is found by summing over all lines. Note that the terms \textit{penalty} and \textit{reward} are used interchangeably, depending on what is most natural. In all cases the reward is the negative of the penalty. The statistics for the current violation of the trained agent and no agent scenario are presented in table \ref{table:results:configuration1_reward_500_episodes_current}

\begin{table}[h]
\center
\caption{Statistics of the current penalty given to the trained agent and no agent scenario over a 500 episodes simulation, each with a duration of 200 hours}
\begin{tabular}{l|rrrrrrr}
         & count    & mean  & std   & 25\%  & 50\%  & 75\%  & max   \\
\hline
Agent    & 1556 & 0.174 & 0.146 & 0.053 & 0.138 & 0.268 & 0.806 \\
No agent & 1487 & 0.161 & 0.142 & 0.045 & 0.121 & 0.250 & 0.787 \\
\hline
\end{tabular}
\label{table:results:configuration1_reward_500_episodes_current}
\end{table}
The trained agent increases the number of current violations by 5 \% from 1487 to 1556. In addition, the mean magnitude of current penalty increases by 9 \%. The agent has not learned an appropriate behaviour to reduce current overloads in the lines. The hours in the simulation that have a current violation if no action is taken are termed critical hours. Figure \ref{fig:results:config1_current_boxplot} shows the distribution of the current penalties for the trained agent and no agent scenario in critical hours. The trained agent is worse because its current penalty distribution is lower than for the no agent scenario.

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_current_boxplot.png}
    \caption {Box plot of the current penalty for the trained agent and no agent scenario in critical hours}
    \label{fig:results:config1_current_boxplot}
\end{figure}
Figure \ref{fig:results:config1_sorted_current} plots the current violations for critical hours in the 500 episode sorted from least to most severe. The hour with the lowest current penalty is the least critical hour. It is clear that the trained agent for the most part aggravates the situation. In fact, it only improves the situation in 9\% of the critical hours, which for the most part are less critical in terms of the size of the current penalty. 

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_sorted_current.png}
    \caption {Current penalties in critical hours sorted from least to most severe for the trained agent and no agent scenario}
    \label{fig:results:config1_sorted_current}
\end{figure}

Figure \ref{fig:results:config1_improvement_current} shows hourly box plots for difference in current reward between the trained agent and the no agent scenario. This figure illustrates which hours of the day the trained agent performs well. For current penalty, the trained agent aggravates the situation in all hours before peak demand. The agent improves the situation in terms of current magnitudes after 18 pm. However, 91 \% of the current violations happen during peak solar production, so the net effect of the improvements are small. 91 \% is also the proportion of times the trained agent makes matters worse in critical hours in terms of the magnitude of the current violations, as shown in figure \ref{fig:results:config1_sorted_current}. Figure \ref{fig:results:config1_bad_current} illustrates a period where current violations occur during peak solar production.

\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_improvement_current.png}
    \caption {Hourly box plots of the difference in current rewards between the trained agent and no agent scenario in critical hours. Positive values means that the trained agent is better}
    \label{fig:results:config1_improvement_current}
\end{figure}



\begin{figure}[h]
    \center
\includegraphics[height=8cm, width=12cm]{figures/config1_bad_current.png}
    \caption {Current reward and solar irradiance profile in a three day period. The agent is not able to prevent current overloads in hours of peak solar irradiance.}
    \label{fig:results:config1_bad_current}
\end{figure}


\subsection{Summary}
The trained agent shows a desired behaviour in terms of voltage magnitudes in periods of peak power demand. The power consumption is decreased in those periods, which keeps the voltage magnitudes above the lower safety margins. However, the trained agent has not found a strategy that keeps voltage magnitudes below upper margins in periods with peak solar production.

The agent performs poorly in terms of avoiding current overloads in the transmission lines. This is mainly a problem in periods of peak solar production, because there were relatively few current overloads in the lines in hours of peek demand. Still, because it is improving the voltage situation during peak demand, it also reduces the current loading in the transmission lines. Therefore, the agents performs well in terms of current in periods of peak demand. The behaviour of the agent in hours of peak demand and peak solar production is summarised in table \ref{table:results:config1_behaviour}.

\begin{table}[h]
\center
\caption{Behaviour of the agent in critical periods in terms of current and voltage safety margins}
\begin{tabular}{l|ll}
                      & Current behaviour     & Voltage behaviour \\
\hline
Peak demand           & Good & Good              \\
Peak solar production & Poor                  & Poor \\
\hline
\end{tabular}
\label{table:results:config1_behaviour}
\end{table}
 A summary of the performance in terms of voltage and current is presented in table \ref{table:results:config1_summary}. Overall, the trained agent reduces the number of voltage and current violations by 14\%. However, it increases the mean penalty by 7 \% due to its poor behaviour in terms of avoiding current violations. Still, the trained agent is better in terms of the total penalty received in the 500 episode simulation because voltage violations are more frequent. The total penalty in terms of both current and voltage is reduced by 8 \%.


\begin{table}[h]
\center
\caption{Summary of number of violations and mean penalty in terms of current and voltage for the trained agent and the no agent scenario}
\begin{tabular}{l|lrrr}
Type      &               & Agent          & No agent       & Change        \\
\hline
Current   & Violations    & 1556            & 1487          & 5\%            \\
          & Mean penalty     & 0,17           & 0,16           & 9 \%         \\
          & Penalty          & 271            & 239            & 14 \%         \\
          &               &                &                &               \\
Voltage   & Violations    & 7253           & 8804           & -18\%          \\
          & Mean  penalty    & 0,035          & 0,037          & -8\%          \\
          & Penalty          & 251.3          & 330.1         & -24 \%        \\
          &               &                &                &               \\
Total     & Violations    & 8809           & 10291          & -14\%          \\
          & Mean penalty     & 0.059          & 0.055          & 7 \%         \\
\textbf{} & \textbf{Penalty} & \textbf{522.3} & \textbf{569.1} & \textbf{-8\%} \\
\hline
\end{tabular}
\label{table:results:config1_summary}
\end{table}

\end{document}