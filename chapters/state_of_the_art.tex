\documentclass[class=book, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkefølge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}

\begin{document}
\section{Reinforcement learning}
Reinforcement learning is a discipline in rapid development. Reinforcement algorithms have gained from the recent success of methods in supervised learning, such as convolutional neural networks (CNN) and  recurrent neural networks (RNN). Dan Cire\c{s}an et al. had a breakthrough in 2010 when they were able to train a dense neural network using back propagation with a graphic processor unit (GPU) instead of a conventional central processor unit (CPU) \cite{DNN_gpu_2010}. The neural network was trained on the MNIST dataset of handwritten digits using image augmentation such as rotations, horizontally shearing and scaling. Their result showed that the time of the back propagation routine was cut by a factor of 40 with the use of the GPU and they achieved a record breaking low error rate of 0.35 \% in the classification task. In 2011 the team of Dan Cire\c{s}an et al. continued the development and presented an implementation of back propagation for CNN using GPU, cutting the training time from months to days \cite{CNN_gpu_2011}.

In 2013, Mnih et al. at DeepMind Technologies implemented a Q-learning algorithm using CNN as function approximators, inspired by the recent success of supervised learning \cite{DQN_Mnih_et_al_2013}. They called their method Deep Q-Network (DQN) and applied it on seven of the Atari 2600 games. DQN was able beat all previous solutions in all games expect for space invaders, and achieved super-human performance in three games. DQN only learns from raw pixel input, without using low dimensional feature engineering of the input values. In addition, the same network architecture and hyper-parameters were used for all games, proving the generality of DQN. Before DQN, most reinforcement learning used linear function approximators for the action-values function because nonlinear approximated had problems with divergence \cite{DQN_Mnih_et_al_2013}. In addition, Tsitsiklis and Van Roy had presented a proof of convergence, in addition to a bound on the approximation error for linear approximators \cite{linear_stable_Tsitsiklis97}.

A problem with using neural networks as a function approximator in online learning is that samples in reinforcement learning are highly correlated. States visited in chronological order are naturally dependent on each other. Therefore, it is difficult to update the network online, since the gradient estimate will suffer from large variance during training, heavily depending on the recent visted states. Mnih et al. avoided this problem by using an experience replay buffer that stores the \textit{N} last tuples $\langle s_{t}, a_{t}, r_{t}, s_{t+1}\rangle$ of experiences made by the agent. The neural network is updated using stochastic gradient descent by drawing random samples from the the experience replay buffer. In addition to avoiding correlated samples, it is a more sample efficient approach as a single experience can be used in different parameter updates \cite{DQN_Mnih_et_al_2013}.


In each time step, Mnih et al. converted the last 4 frames from RGB to grayscale images, reduced them to 84x84 pixels, and stacked into a 84x84x4 picture. This was the state representation that was sent as input to the CNN. The output layer of the neural network was the action-value for all of the actions in that game. Note that the action space in these games all are discrete actions, i.e move right, move left, shoot etc. Until DQN, similar algorithms all relied on some sort of domain knowledge that was used for feature engineering. DQN, on the other hand, learns to extract the relevant features on its own, from a stack of grayscale images, through its neural network function approximator.

In 2015, Mnih et al. tested DQN on more atari games, and outperformed 43 of the 47 games where reinforcement learning research previously had been conducted \cite{mnih2015_humanlevel}. The algorithm had a slight modification to that presented in the 2013 paper. The Bellman equation in \eqref{eq:theory:bellman_equation} is the foundation for calculating the target values used for updating the parameters in the function approximator. However, the parameters of the neural network are used to calculate the target values which in turn are used to calculate the parameter update. As a result, the learning situation can become unstable because the target values are changing as the network parameters are updated. To solve this, they only periodically updated the parameters used for calculating the target, thereby making it more stable during training. 


Although DQN was a great improvement compared to existing methods, it is not able to tackle tasks with a continuous action space. A self-driving car must not only decide to turn left or right, but it must determine the exact angle to turn the wheel. A possible solution is to discretise the action space, for instance by dividing the possible wheel angles into 5 possible categories: $[-90^{o},-45^{o},0^{o},45^{o},90^{o}]$. This is realisable for tasks consisting of few independent action variables, albeit the steering would be very abrupt, but the action space grows exponentially with the number of variables. For instance, imagine a reinforcement task with 10 independent action variables that all should be discretised into 5 categories. The resulting action space is finite, but has a size of $5^{10} \approx 10, 000,000$. The consequence is that the final layer in the neural network in the DQN algorithm must consists of 10 million neurons, which all would have a parameter for all of the neurons in the previous layer, in addition to a bias. The number of parameters in the neural network would easily surpass 1 billion. Therefore, a better approach is to use policy gradient methods that are well suited for giving a continuous action. 


in 2016, Lillicrap et al. presented the deep deterministic policy gradient (DDPG) as an continuous extension of the DQN algorithm  \cite{DBLP:journals/corr/LillicrapHPHETS15}. DDPG is an actor-critic model that builds on the deterministic policy gradient methods presented by Silver at. el in 2014, where it was shown that deterministic policy methods were better than stochastic methods, especially in tasks with a large action space \cite{pmlr-v32-silver14}. The extension made by Lillicrap et al. was to use a neural network, as in DQN, as function approximators. Specifically, they parametrise both the policy (actor) and action-value function (critic) by neural networks. DDPG also use the replay buffer to ensure uncorrelated samples during parameter updates. They use a separate target network for both the actor and the critic, in a similar fashion to DQN where target values parameters only are periodically updated. DDPG was tested on 33 physics tasks with continuous actions, such as the cartpole and pendulum  environment. They trained both on low-dimensional input data, such as angle velocities, acceleration etc, and also using raw pixels from rendering of the environment. DDPG performed well on most of the tasks, and was able to beat a model-based planning algorithm (iLQG) on several occasions, not only using low-dimensional features, but also using raw pixels as input \cite{DBLP:journals/corr/LillicrapHPHETS15}.

The success of neural networks as function approximators continued in 2016 with AlphaGo (Silver et al.), a reinforcement algorithm trained to play the Chinese board game Go \cite{alphago}. Go is a complex game with a large state and action space that computer programs have been struggling to solve for decades. AlphaGo was the first program to beat the current world champion of Go, winning 4 of 5 games against Lee Sedol \cite{Sutton1998}. AlphaGo was not entirely learning from self-play, but was pre-trained using using expert-moves. However, an extention of the method, called AlphaGo Zero, learned entirely from self-play through the use of Monte Carlo tree search and CNN as function approximators. Finally,  Silver et al. introduced AlphaZero, and showed that the same algorithm and neural network architecture could be generalised to learn chess and the Japanese board game Shogi through self-play \cite{alphazero}. AlphaZero was able to beat the current world-record computer programs in all three games, without any domain-specific knowledge or handcrafted features. 

on April 13 2019, OpenAI Five became the first reinforcement algorithm to beat a world champion team in an esport event \cite{openai_dota}. OpenAi Five played against the world champions of Dota 2, a multiplayer online battle arena video game consisting of two teams of 5 playing against each other. The game is highly strategic, where is important to plan and collect resources to able to destroy the opponents main building \cite{dota_wiki}. Once again, the successful reinforcement algorithm uses neural network as function approximators, although the exact algorithm (PPO) is different from DDPG \cite{PPO_openAI_Schulman}. Still, it shows that neural networks have not only been revolutionary in supervised learning, but also in reinforcement learning. OpenAI Five demonstrates one of the main advantages of neural networks: it continually learns from new input data/experiences. OpenAI Five had a solid 10 month real-time training, corresponding to 45 000 year of self-play in Dota-2 \cite{openai_dota}.


\section{Demand response}
Demand response is a subject that is well researched. Vázquez-Canteli and Nagy have conducted a literature search where they make a review of published research concerning the use of reinforcement learning algorithms and demand response \cite{vazquez2019reinforcement}. This section will discuss their findings and its relevance for the research question in this thesis.

The review includes a total 105 articles, mainly from a literature search in Web of Science. The studies have objectives such as minimising energy cost, peak energy, user discomfort. A way of minimising energy cost and peak energy is by shifting the demand to hours of peak solar production, which in turn helps the power system by avoiding voltage and current safety violations. However, the goal of the studies included in Vázquez-Canteli and Nagy's review do not explicitly use the voltages and line current in the reinforcement algorithm in a similar way as this thesis. In contrast, the studies often focuses on the energy system of individual houses or buildings. For instance,  Yang et al. focus on controlling a heat pumps, geothermal boreholes and PV modules to minimise a building's energy usage \cite{yang2015reinforcement}.  Dusparic et al. use reinforcement learning to control the charging of electrical vehicles such that the current limit for a transformer is not violated \cite{dusparic2013multi}. Gemine et al. have formulated a similar problem as the research question in this thesis, allowing the agent to not only change the consumption in but also to curtail generation of power \cite{active_network_management}. However, they do not solve the task using a reinforcement learning algorithm, although they formulate the problem as a Markov decision process, nor do they consider continuous actions. 

has been used to control several many tasks 
- Electricity for cooling: air conditioning (HVAC) and domestic hot water (DHW)
Flexibility possibilities: Home appliances, e.g. dishwashers, washing machines, dryers,
electric vehicles (EVs), batteries. 

-Available flexibility in USA 2015: 6,6 \% in peak demand

-Demand elasticity: extremely low.
\cite{active_network_management}
- Advantages with DR
1. Improved grid stability due to increased demand flexibility.
2. Shift of peak demand towards periods of peak renewable energy
generation.
3. Lower thermal costs and electricity prices. Since the peak to average
ratio of the demand decreases, less peaking plants need to be operated.
4. Reduction of the investments in generation, transmission, and distribution assets, which are sized to meet peak demand.
5. Lower capacity reserves requirements.
6. Reduced energy bills for consumers.

- Similar studies, same grid effect, but different RL implementation:


. Wang et al. used actor-critic RL to maximize occupants’
thermal comfort and minimize the energy consumption of the HVAC
system in a simulated office building

. Claessens et al. used BRL in
two simulated case studies for peak demand minimization and to reduce the cost of electricity

.Dusparic et al. proposed a control system for the charging process of
9 EVs in a neighborhood in which all the houses were connected to the
same transformer [68]. ISH THE SAME WITH TRAFO

. Kofinas et al. proposed a decentralized cooperative multi-agent
RL algorithm to maximize the self-energy consumption in a microgrid
[150]. Microgrid with PV, several agents decides how much power to draw from the PV production.
Does not monitor voltages/current, as I do. 

. Tan et al. used a centralized controller based on Q-learning
to reduce the operational cost of an electric microgrid [151].



\end{document}