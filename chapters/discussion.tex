\documentclass[class=book, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.2in]{geometry}
\usepackage[sorting = none,
            doi = true  %lesedato for url-adresse
            ]{biblatex} %none gir bibliografi i sitert rekkef√∏lge
\addbibresource{reference.bib}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\pgfplotsset{compat=1.15}

\begin{document}
\section{Performance of the trained agent}
The result from section \ref{section:result:config1} show that the trained agent is able to reduce the number of safety violation by 17\%. However, the trained agent is only able to reduce violations of voltage safety margins, not current violations. In fact, it increases the number of current violations by 8 \%. Still, there are large differences in terms of the quantity and magnitude of the current and voltage safety violations. There are over 5 times more voltage violations than current violations. This is of course dependent on the voltage bounds that are used for defining the voltage cost. In this case the voltage bounds are chosen to be 0.95 and 1.05 pu. Although there are many more voltage violations in a normal day, the average current violation is more severe. Specifically, the mean current cost is almost 3 times greater than the mean voltage cost. The nature of the transmission in terms of violations is therefore: the current violation are few and severe, while the voltage violations are numerous and faint.

Why is the trained agent better at avoiding voltage violation than current violations? A possible reason is that the agent is penalised more for voltage violations on average. The cost in the no agent scenario for configuration 1 is 25 \% higher in terms of voltage. It is logical that the agent learns a behaviour that reduces the most punishing term, namely the voltage cost. As stated in section \label{section:config1:current_violations}, the trained agent only learned the appropriate behaviour in periods of peak demand, i.e reducing the demand so that less power need to be imported from the grid.

The agent was worsening the situation in periods of peak solar production. The desired behaviour in such a situation is to increase the demand, so that less excess solar power needs to be exported to the grid. Consequently, the trained agent decreases the demand in periods of peak solar production, since the line overload increases. It is possible that the learned behaviour simply is to decrease the demand at all times. However, the agent is not always decreasing the demand, as shown is figure \ref{fig:results:configuration1}. Still, the actions of the agent at each load is not telling for the consequences it has on the grid. The action determines the percentage change in demand at each load. Naturally, the load is varying a lot throghout a day, but the nominal demand also vary a lot from load to load.
\end{document}